---
title: Learning from Networked Examples
abstract: Many machine learning algorithms are based on the assumption that training
  examples are drawn independently.  However, this assumption does not hold anymore
  when learning from a networked sample because two or more training examples may
  share some common objects,  and hence share the features of these shared objects.  We
  show that the classic approach of ignoring this problem potentially can have a harmful
  effect on the accuracy of statistics, and then consider alternatives.  One of these
  is to only use independent examples, discarding other information.  However, this
  is clearly suboptimal.  We analyze sample error bounds in this networked setting,
  providing significantly improved results.  An important component of our approach
  is formed by efficient sample weighting schemes, which leads to novel concentration
  inequalities.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wang17a
month: 0
tex_title: Learning from Networked Examples
firstpage: 641
lastpage: 666
page: 641-666
order: 641
cycles: false
author:
- given: Yuyi
  family: Wang
- given: Zheng-Chu
  family: Guo
- given: Jan
  family: Ramon
date: 2017-10-11
address: 
publisher: PMLR
container-title: Proceedings of the 28th International Conference on Algorithmic Learning
  Theory
volume: '76'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 10
  - 11
pdf: http://proceedings.mlr.press/v76/wang17a/wang17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
