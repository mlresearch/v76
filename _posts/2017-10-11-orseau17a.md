---
title: 'Soft-Bayes: Prod for Mixtures of Experts with Log-Loss'
abstract: We consider prediction with expert advice under the log-loss with the goal
  of deriving efficient and robust algorithms. We argue that existing algorithms such
  as exponentiated gradient, online gradient descent and online Newton step do not
  adequately satisfy both requirements. Our main contribution is an analysis of the
  Prod algorithm that is robust to any data sequence and runs in linear time relative
  to the number of experts in each round. Despite the unbounded nature of the log-loss,
  we derive a bound that is independent of the largest loss and of the largest gradient,
  and depends only on the number of experts and the time horizon. Furthermore we give
  a Bayesian interpretation of Prod and adapt the algorithm to derive a tracking regret.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: orseau17a
month: 0
tex_title: 'Soft-Bayes: Prod for Mixtures of Experts with Log-Loss'
firstpage: 372
lastpage: 399
page: 372-399
order: 372
cycles: false
author:
- given: Laurent
  family: Orseau
- given: Tor
  family: Lattimore
- given: Shane
  family: Legg
date: 2017-10-11
address: 
publisher: PMLR
container-title: Proceedings of the 28th International Conference on Algorithmic Learning
  Theory
volume: '76'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 10
  - 11
pdf: http://proceedings.mlr.press/v76/orseau17a/orseau17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
