---
title: Relative Error Embeddings of the Gaussian Kernel Distance
abstract: A reproducing kernel defines an embedding of a data point into an infinite dimensional reproducing kernel Hilbert space (RKHS).  The norm in this space describes a distance, which we call the kernel distance.  The random Fourier features (of Rahimi and Recht) describe an oblivious approximate mapping into finite dimensional Euclidean space that behaves similar to the RKHS.  We show in this paper that for the Gaussian kernel the Euclidean norm between these mapped to features has $(1+\varepsilon)$-relative error with respect to the kernel distance.  When there are $n$ data points, we show that $O((1/\varepsilon^2) \log n)$ dimensions of the approximate feature space are sufficient and necessary. Without a bound on $n$, but when the original points lie in $\mathbb{R}^d$ and have diameter bounded by $\mathcal{M}$, then we show that $O((d/\varepsilon^2) \log \mathcal{M})$ dimensions are sufficient, and that this many are required, up to $\log(1/\varepsilon)$ factors.	We empirically confirm that relative error is indeed preserved for kernel PCA using these approximate feature maps.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: chen17a
month: 0
tex_title: Relative Error Embeddings of the Gaussian Kernel Distance
firstpage: 560
lastpage: 576
page: 560-576
order: 560
cycles: false
author:
- given: Di
  family: Chen
- given: Jeff M.
  family: Phillips
date: 2017-10-11
address: 
publisher: PMLR
container-title: Proceedings of the 28th International Conference on Algorithmic Learning
  Theory
volume: '76'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 10
  - 11
pdf: http://proceedings.mlr.press/v76/chen17a/chen17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
