---
title: Universality of Bayesian mixture predictors
abstract: 'The problem is that of sequential probability forecasting for discrete-valued
  time series. The data is generated by  an unknown  probability distribution over
  the space of all one-way infinite sequences. It is known that this measure belongs
  to a given set $\mathcalC$, but the latter is completely arbitrary  (uncountably
  infinite, without any structure given).  The performance is measured  by  asymptotic
  average log loss.  In this work it is shown that the minimax asymptotic performance  is
  always attainable, and it is attained by a Bayesian mixture over countably many  measures
  from the set $\mathcalC$. This was previously only known for the case when the best
  achievable asymptotic error isÂ 0. The new result can  be interpreted  as a complete-class
  theorem for prediction.  It also contrasts previous results that show that in the
  non-realizable case all Bayesian mixtures may be suboptimal. This leads to a very
  general conclusion concerning model selection for a problem of sequential inference:
  it is better to take a model large enough to make sure it includes the process that
  generates the data, even if it entails positive asymptotic average loss, for otherwise
  any combination of predictors in the model  class may be useless.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ryabko17a
month: 0
tex_title: Universality of Bayesian mixture predictors
firstpage: 57
lastpage: 71
page: 57-71
order: 57
cycles: false
author:
- given: Daniil
  family: Ryabko
date: 2017-10-11
address: 
publisher: PMLR
container-title: Proceedings of the 28th International Conference on Algorithmic Learning
  Theory
volume: '76'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 10
  - 11
pdf: http://proceedings.mlr.press/v76/ryabko17a/ryabko17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
